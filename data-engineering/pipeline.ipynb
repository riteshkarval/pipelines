{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dkube.sdk import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "user = os.getenv('USERNAME', 'songole')\n",
    "code_name = \"tmdb\"\n",
    "merge_ds = \"tmdb-merged\"\n",
    "clean_ds = \"tmdb-cleaned\"\n",
    "train_fs = \"tmdb-train-fs\"\n",
    "test_fs  = \"tmdb-test-fs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DkubeApi(token=existing_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding code tmdb\n",
      "{'code': 200, 'message': 'Added code tmdb-3mCc successfully', 'uuid': None}\n",
      "code tmdb - completed with state READY and reason None\n",
      "Code tmdb added\n",
      "Adding dataset tmdb-merged\n",
      "{'code': 200, 'message': 'Added dataset tmdb-merged-OP1r successfully', 'uuid': None}\n",
      "dataset tmdb-merged - completed with state READY and reason None\n",
      "Dataset tmdb-merged added\n",
      "Adding dataset tmdb-cleaned\n",
      "{'code': 200, 'message': 'Added dataset tmdb-cleaned-wd5x successfully', 'uuid': None}\n",
      "dataset tmdb-cleaned - completed with state READY and reason None\n",
      "Dataset tmdb-cleaned added\n",
      "Adding featureset tmdb-train-fs\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "Featureset tmdb-train-fs added\n",
      "Adding featureset tmdb-test-fs\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "create_featureset: waiting for featureset to be setup\n",
      "Featureset tmdb-test-fs added\n"
     ]
    }
   ],
   "source": [
    "print(f\"Adding code {code_name}\")\n",
    "code = DkubeCode(user, name=code_name)\n",
    "code.update_git_details(\"https://github.com/riteshkarvaloc/pipelines.git\")\n",
    "api.create_code(code)\n",
    "print(f\"Code {code_name} added\")\n",
    "\n",
    "print(f\"Adding dataset {merge_ds}\")\n",
    "dataset = DkubeDataset(user, name=merge_ds)\n",
    "dataset.update_dataset_source(source='dvs')\n",
    "api.create_dataset(dataset)\n",
    "print(f\"Dataset {merge_ds} added\")\n",
    "\n",
    "print(f\"Adding dataset {clean_ds}\")\n",
    "dataset = DkubeDataset(user, name=clean_ds)\n",
    "dataset.update_dataset_source(source='dvs')\n",
    "api.create_dataset(dataset)\n",
    "print(f\"Dataset {clean_ds} added\")\n",
    "\n",
    "print(f\"Adding featureset {train_fs}\")\n",
    "featureset = DkubeFeatureSet(name=train_fs)\n",
    "api.create_featureset(featureset)\n",
    "print(f\"Featureset {train_fs} added\")\n",
    "\n",
    "print(f\"Adding featureset {test_fs}\")\n",
    "featureset = DkubeFeatureSet(name=test_fs)\n",
    "api.create_featureset(featureset)\n",
    "print(f\"Featureset {test_fs} added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import random\n",
    "import string\n",
    "\n",
    "generate = lambda hint: \"{}-{}\".format(hint, ''.join([random.choice(string.digits) for n in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(existing_token=existing_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"docker.io/ocdr/dkube-datascience-tf-cpu:v2.0.0-3\"\n",
    "merge_ds_path = \"/data/merge\"\n",
    "clean_ds_path = \"/data/clean\"\n",
    "test_fs_path = \"/data/test_fs\"\n",
    "train_fs_path = \"/data/train_fs\"\n",
    "merge_script = \"cd data-engineering; python merging.py\"\n",
    "clean_script = \"cd data-engineering; python cleaning.py\"\n",
    "feature_script = f\"cd data-engineering; python feature-engineering.py --train_fs {train_fs} --test_fs {test_fs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_url = \"/mnt/dkube/pipeline/components/\"\n",
    "dkube_preprocessing_op = kfp.components.load_component_from_file(components_url + \"preprocess/component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='dkube-feature-engineering-pl',\n",
    "    description='example titanic pipeline to submit to leaderboard'\n",
    ")\n",
    "def data_engineering_pipeline(token):\n",
    "\n",
    "    merge = dkube_preprocessing_op(token, json.dumps({\"image\": image}),\n",
    "                                   program=code_name, run_script=merge_script,\n",
    "                                   outputs=json.dumps([str(merge_ds)]),\n",
    "                                   output_mounts=json.dumps([merge_ds_path])).set_display_name(\"Merge\")\n",
    "    \n",
    "    clean = dkube_preprocessing_op(token, json.dumps({\"image\": image}),\n",
    "                                   program=code_name, run_script=clean_script,\n",
    "                                   datasets = json.dumps([str(merge_ds)]),\n",
    "                                   input_dataset_mounts = json.dumps([merge_ds_path]),\n",
    "                                   outputs=json.dumps([str(clean_ds)]),\n",
    "                                   output_mounts=json.dumps([clean_ds_path])).after(merge).set_display_name(\"clean\")\n",
    "    \n",
    "    f_eng = dkube_preprocessing_op(token, json.dumps({\"image\": image}),\n",
    "                                   program=code_name, run_script=feature_script,\n",
    "                                   datasets=json.dumps([str(clean_ds)]), \n",
    "                                   output_featuresets=json.dumps([train_fs, test_fs]),\n",
    "                                   input_dataset_mounts=json.dumps([clean_ds_path]), \n",
    "                                   output_featureset_mounts=json.dumps([train_fs_path, test_fs_path])\n",
    "                                    ).after(clean).set_display_name(\"Feature-Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/3c346a0f-13c2-47b8-81cb-12b61de5d1ea\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'Dkube- DE pl'\n",
    "de_experiment = client.create_experiment(name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-6be473a58497>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6be473a58497>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    try\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import kfp.compiler as compiler\n",
    "arguments = {\"token\":existing_token}\n",
    "compiler.Compiler().compile(data_engineering_pipeline, \"de-pipeline.zip\")\n",
    "try:\n",
    "    pipeline = client.upload_pipeline(\"de-pipeline.zip\", pipeline_name = \"data-engineering-pipeline\")\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(de_experiment.id, job_name=f\"[{pipeline.name}] Run\" + str(runid), pipeline_id=pipeline.id, params=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
